{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is7oSuxiyUDs"
      },
      "outputs": [],
      "source": [
        "!pip install -q ultralytics kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "dR8r_2wXyzlK",
        "outputId": "f893a2c6-ec7c-49bf-866e-a1fd0dc15770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2932dc86-ec68-4e54-88fc-037429fa7003\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2932dc86-ec68-4e54-88fc-037429fa7003\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"hasnain1234\",\"key\":\"60e1947e8eacbc53ddb7727533744614\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshaymiddha/crack-segmentation-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxVCzz9RyzhW",
        "outputId": "85e4a2f6-9a8c-4432-e918-182d234aa8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/crack-segmentation-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/input/crack-segmentation-dataset/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WiFH8HM0kUn",
        "outputId": "286f7375-df68-4d65-86af-400839a1afe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/kaggle/input/crack-segmentation-dataset/test': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/lakshaymiddha/crack-segmentation-dataset/versions/1/crack_segmentation_dataset/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4ghAmVA08jm",
        "outputId": "d9c7bc44-12dd-4a1d-a13e-01efbc90753a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images\tlabels\tlabels.cache  masks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Dataset path (change to \"train\" if needed)\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/lakshaymiddha/crack-segmentation-dataset/versions/1/crack_segmentation_dataset/train/images\"\n",
        "\n",
        "# List all files containing 'noncrack' in their name\n",
        "noncrack_images = [\n",
        "    f for f in os.listdir(dataset_path)\n",
        "    if \"noncrack\" in f.lower() and f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
        "]\n",
        "\n",
        "# Print results\n",
        "print(f\"Total noncrack images in test folder: {len(noncrack_images)}\")\n",
        "print(\"Sample noncrack image filenames:\")\n",
        "print(noncrack_images[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcRsutyw1Pjn",
        "outputId": "777f71ae-1846-420f-fbc3-36106b1939cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total noncrack images in test folder: 1199\n",
            "Sample noncrack image filenames:\n",
            "['noncrack_noncrack_concrete_wall_85_0.jpg.jpg', 'noncrack_noncrack_concrete_wall_77_4.jpg.jpg', 'noncrack_noncrack_concrete_wall_60_14.jpg.jpg', 'noncrack_noncrack_concrete_wall_74_29.jpg.jpg', 'noncrack_noncrack_concrete_wall_16_6.jpg.jpg', 'noncrack_noncrack_concrete_wall_24_21.jpg.jpg', 'noncrack_noncrack_concrete_wall_45_6.jpg.jpg', 'noncrack_noncrack_concrete_wall_9_15.jpg.jpg', 'noncrack_noncrack_concrete_wall_16_0.jpg.jpg', 'noncrack_noncrack_concrete_wall_60_16.jpg.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def convert_masks_to_yolo_seg(image_dir, mask_dir, label_dir):\n",
        "    \"\"\"\n",
        "    Convert binary mask images to YOLO segmentation format.\n",
        "\n",
        "    Args:\n",
        "        image_dir: Directory containing original images\n",
        "        mask_dir: Directory containing binary mask images (white=crack, black=background)\n",
        "        label_dir: Output directory for YOLO format label files\n",
        "    \"\"\"\n",
        "    os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "    # Get all jpg files from mask directory\n",
        "    files = [f for f in os.listdir(mask_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    processed = 0\n",
        "    empty_masks = 0\n",
        "\n",
        "    for filename in files:\n",
        "        mask_path = os.path.join(mask_dir, filename)\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "        label_path = os.path.join(label_dir, filename.rsplit('.', 1)[0] + '.txt')\n",
        "\n",
        "        # Read mask image\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            print(f\"Warning: Could not read {mask_path}\")\n",
        "            continue\n",
        "\n",
        "        height, width = mask.shape\n",
        "\n",
        "        # Create binary mask (white pixels = 255 become crack regions)\n",
        "        _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Check if mask is empty (no cracks detected)\n",
        "        if cv2.countNonZero(binary_mask) == 0:\n",
        "            # Create empty label file for images without cracks\n",
        "            with open(label_path, 'w') as f:\n",
        "                pass  # Empty file\n",
        "            empty_masks += 1\n",
        "            continue\n",
        "\n",
        "        # Find contours of crack regions\n",
        "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        with open(label_path, \"w\") as f:\n",
        "            for contour in contours:\n",
        "                # Simplify contour to reduce points while preserving shape\n",
        "                epsilon = 0.002 * cv2.arcLength(contour, True)  # Adjust epsilon for more/less simplification\n",
        "                simplified_contour = cv2.approxPolyDP(contour, epsilon, True)\n",
        "\n",
        "                # Skip very small contours (likely noise)\n",
        "                if len(simplified_contour) < 3:\n",
        "                    continue\n",
        "\n",
        "                # Flatten the contour array\n",
        "                points = simplified_contour.reshape(-1, 2)\n",
        "\n",
        "                # Skip if contour is too small\n",
        "                if len(points) < 3:\n",
        "                    continue\n",
        "\n",
        "                # Normalize coordinates to [0, 1] range\n",
        "                normalized_points = []\n",
        "                for x, y in points:\n",
        "                    norm_x = max(0, min(1, x / width))   # Clamp to [0, 1]\n",
        "                    norm_y = max(0, min(1, y / height))  # Clamp to [0, 1]\n",
        "                    normalized_points.extend([norm_x, norm_y])\n",
        "\n",
        "                # Write YOLO segmentation format: class_id x1 y1 x2 y2 x3 y3 ...\n",
        "                # Class 0 for crack\n",
        "                coords_str = \" \".join(f\"{coord:.6f}\" for coord in normalized_points)\n",
        "                f.write(f\"0 {coords_str}\\n\")\n",
        "\n",
        "        processed += 1\n",
        "        if processed % 100 == 0:\n",
        "            print(f\"Processed {processed} files...\")\n",
        "\n",
        "    print(f\"Processed {processed} files, {empty_masks} had no cracks\")\n",
        "\n",
        "def main():\n",
        "    # Dataset paths\n",
        "    base_path = \"/root/.cache/kagglehub/datasets/lakshaymiddha/crack-segmentation-dataset/versions/1/crack_segmentation_dataset\"\n",
        "    splits = [\"train\", \"test\"]\n",
        "\n",
        "    for split in splits:\n",
        "        print(f\"\\nProcessing {split} split...\")\n",
        "        image_dir = os.path.join(base_path, split, \"images\")\n",
        "        mask_dir = os.path.join(base_path, split, \"masks\")\n",
        "        label_dir = os.path.join(base_path, split, \"labels\")\n",
        "\n",
        "        # Check if directories exist\n",
        "        if not os.path.exists(image_dir):\n",
        "            print(f\"Warning: Image directory not found: {image_dir}\")\n",
        "            continue\n",
        "        if not os.path.exists(mask_dir):\n",
        "            print(f\"Warning: Mask directory not found: {mask_dir}\")\n",
        "            continue\n",
        "\n",
        "        convert_masks_to_yolo_seg(image_dir, mask_dir, label_dir)\n",
        "\n",
        "    print(\"\\n✅ YOLO segmentation label generation completed!\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgblCjBMGJja",
        "outputId": "89c8ddbf-e949-4e1a-ba00-1fd41bb85fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing train split...\n",
            "Processed 100 files...\n",
            "Processed 200 files...\n",
            "Processed 300 files...\n",
            "Processed 400 files...\n",
            "Processed 500 files...\n",
            "Processed 600 files...\n",
            "Processed 700 files...\n",
            "Processed 800 files...\n",
            "Processed 900 files...\n",
            "Processed 1000 files...\n",
            "Processed 1100 files...\n",
            "Processed 1200 files...\n",
            "Processed 1300 files...\n",
            "Processed 1400 files...\n",
            "Processed 1500 files...\n",
            "Processed 1600 files...\n",
            "Processed 1700 files...\n",
            "Processed 1800 files...\n",
            "Processed 1900 files...\n",
            "Processed 2000 files...\n",
            "Processed 2100 files...\n",
            "Processed 2200 files...\n",
            "Processed 2300 files...\n",
            "Processed 2400 files...\n",
            "Processed 2500 files...\n",
            "Processed 2600 files...\n",
            "Processed 2700 files...\n",
            "Processed 2800 files...\n",
            "Processed 2900 files...\n",
            "Processed 3000 files...\n",
            "Processed 3100 files...\n",
            "Processed 3200 files...\n",
            "Processed 3300 files...\n",
            "Processed 3400 files...\n",
            "Processed 3500 files...\n",
            "Processed 3600 files...\n",
            "Processed 3700 files...\n",
            "Processed 3800 files...\n",
            "Processed 3900 files...\n",
            "Processed 4000 files...\n",
            "Processed 4100 files...\n",
            "Processed 4200 files...\n",
            "Processed 4300 files...\n",
            "Processed 4400 files...\n",
            "Processed 4500 files...\n",
            "Processed 4600 files...\n",
            "Processed 4700 files...\n",
            "Processed 4800 files...\n",
            "Processed 4900 files...\n",
            "Processed 5000 files...\n",
            "Processed 5100 files...\n",
            "Processed 5200 files...\n",
            "Processed 5300 files...\n",
            "Processed 5400 files...\n",
            "Processed 5500 files...\n",
            "Processed 5600 files...\n",
            "Processed 5700 files...\n",
            "Processed 5800 files...\n",
            "Processed 5900 files...\n",
            "Processed 6000 files...\n",
            "Processed 6100 files...\n",
            "Processed 6200 files...\n",
            "Processed 6300 files...\n",
            "Processed 6400 files...\n",
            "Processed 6500 files...\n",
            "Processed 6600 files...\n",
            "Processed 6700 files...\n",
            "Processed 6800 files...\n",
            "Processed 6900 files...\n",
            "Processed 7000 files...\n",
            "Processed 7100 files...\n",
            "Processed 7200 files...\n",
            "Processed 7300 files...\n",
            "Processed 7400 files...\n",
            "Processed 7500 files...\n",
            "Processed 7600 files...\n",
            "Processed 7700 files...\n",
            "Processed 7800 files...\n",
            "Processed 7900 files...\n",
            "Processed 8000 files...\n",
            "Processed 8100 files...\n",
            "Processed 8200 files...\n",
            "Processed 8300 files...\n",
            "Processed 8370 files, 1233 had no cracks\n",
            "\n",
            "Processing test split...\n",
            "Processed 100 files...\n",
            "Processed 200 files...\n",
            "Processed 300 files...\n",
            "Processed 400 files...\n",
            "Processed 500 files...\n",
            "Processed 600 files...\n",
            "Processed 700 files...\n",
            "Processed 800 files...\n",
            "Processed 900 files...\n",
            "Processed 1000 files...\n",
            "Processed 1100 files...\n",
            "Processed 1200 files...\n",
            "Processed 1300 files...\n",
            "Processed 1400 files...\n",
            "Processed 1474 files, 221 had no cracks\n",
            "\n",
            "✅ YOLO segmentation label generation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# AGGRESSIVE COLAB CRASH PREVENTION\n",
        "# =============================================================================\n",
        "import gc\n",
        "import torch\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "# Force garbage collection multiple times\n",
        "for _ in range(3):\n",
        "    gc.collect()\n",
        "\n",
        "# Clear all CUDA memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()  # Clear IPC memory\n",
        "    print(\"✅ GPU memory cleared\")\n",
        "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Check memory\n",
        "print(f\"Current RAM usage: {psutil.virtual_memory().percent:.1f}%\")\n",
        "print(f\"Available RAM: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
        "\n",
        "# Set environment variables for memory optimization\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ULTRA-CONSERVATIVE SETTINGS FOR COLAB\n",
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "\n",
        "model.train(\n",
        "    data=\"crackSegment.yaml\",\n",
        "    epochs=16,                      # REDUCED epochs to prevent timeout\n",
        "    imgsz=256,                      # SMALLER image size (was 320)\n",
        "    batch=8,                       # SMALLER batch\n",
        "    cache=False,                    # NO image caching\n",
        "    device='cuda',\n",
        "    plots=False,\n",
        "    name=\"segmentCracksYOLOv8n-seg\",\n",
        "\n",
        "    # Memory-efficient settings\n",
        "    amp=True,                       # Mixed precision\n",
        "    patience=4,                     # Early stopping\n",
        "\n",
        "    # Other optimizations\n",
        "    verbose=False,                  # Less printing\n",
        "    deterministic=False,           # Faster training\n",
        "    single_cls=True                # Single class optimization\n",
        ")\n",
        "\n",
        "print(\"✅ Training completed successfully!\")\n",
        "\n",
        "# Final cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_4xFLFV1PgV",
        "outputId": "3598814b-bd07-4d3e-8fda-95326014592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU memory cleared\n",
            "GPU Memory available: 15.8 GB\n",
            "Current RAM usage: 24.5%\n",
            "Available RAM: 10.3 GB\n",
            "Ultralytics 8.3.154 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=crackSegment.yaml, degrees=0.0, deterministic=False, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=16, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=segmentCracksYOLOv8n-seg7, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=4, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/segment/segmentCracksYOLOv8n-seg7, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding class names with single class.\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
            "YOLOv8n-seg summary: 151 layers, 3,263,811 parameters, 3,263,795 gradients, 12.1 GFLOPs\n",
            "\n",
            "Transferred 381/417 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.1±0.1 ms, read: 415.8±523.0 MB/s, size: 49.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/.cache/kagglehub/datasets/lakshaymiddha/crack-segmentation-dataset/versions/1/crack_segmentation_dataset/train/labels.cache... 9603 images, 10836 backgrounds, 0 corrupt: 100%|██████████| 19206/19206 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 616.8±763.9 MB/s, size: 38.1 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /root/.cache/kagglehub/datasets/lakshaymiddha/crack-segmentation-dataset/versions/1/crack_segmentation_dataset/test/labels.cache... 1695 images, 1916 backgrounds, 0 corrupt: 100%|██████████| 3390/3390 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
            "Image sizes 256 train, 256 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/segment/segmentCracksYOLOv8n-seg7\u001b[0m\n",
            "Starting training for 16 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/16     0.379G      1.829       2.82      2.152      1.486         15        256: 100%|██████████| 2401/2401 [08:16<00:00,  4.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:26<00:00,  8.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       3390      32088      0.133     0.0413     0.0199     0.0103     0.0897     0.0271     0.0092    0.00208\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/16     0.695G      1.785       2.63      1.903       1.47         16        256: 100%|██████████| 2401/2401 [07:53<00:00,  5.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:26<00:00,  8.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       3390      32088      0.182     0.0438      0.027     0.0129      0.125     0.0327     0.0156    0.00362\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/16     0.695G      1.713      2.574      1.815      1.439         10        256: 100%|██████████| 2401/2401 [07:51<00:00,  5.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:25<00:00,  8.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       3390      32088      0.198     0.0433     0.0291     0.0155      0.151     0.0317     0.0188    0.00431\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/16     0.697G      1.672      2.547      1.762      1.411         49        256:  77%|███████▋  | 1844/2401 [06:05<01:35,  5.83it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxJe-22r1PcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}